{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Rooms Type      Price Method  Distance  Bedroom2  Bathroom  Car  Landsize  \\\n0      2    h  1480000.0      S       2.5       2.0       1.0  1.0     202.0   \n1      2    h  1035000.0      S       2.5       2.0       1.0  0.0     156.0   \n2      3    h  1465000.0     SP       2.5       3.0       2.0  0.0     134.0   \n3      3    h   850000.0     PI       2.5       3.0       2.0  1.0      94.0   \n4      4    h  1600000.0     VB       2.5       3.0       1.0  2.0     120.0   \n\n   BuildingArea  YearBuilt CouncilArea  Propertycount  \n0           NaN        NaN       Yarra         4019.0  \n1          79.0     1900.0       Yarra         4019.0  \n2         150.0     1900.0       Yarra         4019.0  \n3           NaN        NaN       Yarra         4019.0  \n4         142.0     2014.0       Yarra         4019.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rooms</th>\n      <th>Type</th>\n      <th>Price</th>\n      <th>Method</th>\n      <th>Distance</th>\n      <th>Bedroom2</th>\n      <th>Bathroom</th>\n      <th>Car</th>\n      <th>Landsize</th>\n      <th>BuildingArea</th>\n      <th>YearBuilt</th>\n      <th>CouncilArea</th>\n      <th>Propertycount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>h</td>\n      <td>1480000.0</td>\n      <td>S</td>\n      <td>2.5</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>202.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Yarra</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>h</td>\n      <td>1035000.0</td>\n      <td>S</td>\n      <td>2.5</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>156.0</td>\n      <td>79.0</td>\n      <td>1900.0</td>\n      <td>Yarra</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>h</td>\n      <td>1465000.0</td>\n      <td>SP</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>134.0</td>\n      <td>150.0</td>\n      <td>1900.0</td>\n      <td>Yarra</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>h</td>\n      <td>850000.0</td>\n      <td>PI</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>94.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Yarra</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>h</td>\n      <td>1600000.0</td>\n      <td>VB</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>120.0</td>\n      <td>142.0</td>\n      <td>2014.0</td>\n      <td>Yarra</td>\n      <td>4019.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/raw/melb_data.csv' , usecols=['Rooms', 'Type', 'Price', 'Method', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Propertycount'])\n",
    "df.Landsize = (df.Landsize).replace(0, np.nan)\n",
    "df.BuildingArea = (df.BuildingArea).replace(0, np.nan)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13580 entries, 0 to 13579\nData columns (total 13 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Rooms          13580 non-null  int64  \n 1   Type           13580 non-null  object \n 2   Price          13580 non-null  float64\n 3   Method         13580 non-null  object \n 4   Distance       13580 non-null  float64\n 5   Bedroom2       13580 non-null  float64\n 6   Bathroom       13580 non-null  float64\n 7   Car            13518 non-null  float64\n 8   Landsize       11641 non-null  float64\n 9   BuildingArea   7113 non-null   float64\n 10  YearBuilt      8205 non-null   float64\n 11  CouncilArea    12211 non-null  object \n 12  Propertycount  13580 non-null  float64\ndtypes: float64(9), int64(1), object(3)\nmemory usage: 1.3+ MB\n"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Rooms               0\nType                0\nPrice               0\nMethod              0\nDistance            0\nBedroom2            0\nBathroom            0\nCar                62\nLandsize         1939\nBuildingArea     6467\nYearBuilt        5375\nCouncilArea      1369\nPropertycount       0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6 Different ways to Handling Missing Values\n",
    "\n",
    "<h2>1. Do Nothing</h2>\n",
    "<ul>\n",
    "    <li>We just let the algorithm handle the missing data\n",
    "    <li>This only works for several algorithm (ie. XGBoost and LightGBM)\n",
    "    <li>XGBoost can factor in the missing values and learn the best imputation values for the missing data based on the training loss reduction\n",
    "    <li>LightGBM has the option to just ignore them (<i>use_missing=false</i>)\n",
    "    <li>However, other algorithms will panic and throw an error complaining about the missing values (ie. Scikit learn — LinearRegression) so we need to handle the missing data and clean it before feeding it to the algorithm\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>2. Remove rows</h2>\n",
    "<ul>\n",
    "    <li>The simplest approach for dealing with missing values is to remove entire predictor(s) and/or sample(s) that contain missing values\n",
    "    <li>If we want to to this, please make sure we just remove maximum 3% from our total data observations\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "original observations : 13580\nafter remove Null values : 13518\nTotal observations were removed : 0.4566%\n"
    }
   ],
   "source": [
    "df_test = df.copy()\n",
    "df_test.dropna(subset=['Car'], axis=0, inplace=True)\n",
    "total = ((df.shape[0] - df_test.shape[0])/df.shape[0]) * 100\n",
    "print('original observations : {}\\nafter remove Null values : {}\\nTotal observations were removed : {:.4f}%'.format(df.shape[0], df_test.shape[0], total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>3. Imputation using Mean/Median values</h2>\n",
    "<strong>Pros</strong><ul>\n",
    "    <li>Easy and fast\n",
    "    <li>Works well with small numerical datasets\n",
    "</ul>\n",
    "<strong>Cons</strong><ul>\n",
    "    <li>Doesn’t factor the correlations between features, it only works on the column level\n",
    "    <li>Will give poor results on encoded categorical features\n",
    "    <li>Not very accurate\n",
    "    <li>Doesn’t account for the uncertainty in the imputations\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count    13580.000000\nmean         1.611856\nstd          0.960793\nmin          0.000000\n25%          1.000000\n50%          2.000000\n75%          2.000000\nmax         10.000000\nName: Car, dtype: float64"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df_test = df.copy()\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputer.fit(df_test[['Car']])\n",
    "df_test[['Car']] = imputer.transform(df_test[['Car']])\n",
    "df_test.Car.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>4. Imputation using Most Frequent or Zero/Constant values</h2>\n",
    "<strong>Pros</strong><ul>\n",
    "    <li>Works well with categorical features\n",
    "</ul>\n",
    "<strong>Cons</strong><ul>\n",
    "    <li>Doesn’t factor the correlations between features, it only works on the column level\n",
    "    <li>It can introduce bias in the data\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count    13580.000000\nmean         1.611856\nstd          0.960793\nmin          0.000000\n25%          1.000000\n50%          2.000000\n75%          2.000000\nmax         10.000000\nName: Car, dtype: float64"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df_test = df.copy()\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer.fit(df_test[['Car']])\n",
    "df_test[['Car']] = imputer.transform(df_test[['Car']])\n",
    "df_test.Car.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>5. Imputation using k-NN</h2>\n",
    "<strong>Pros</strong><ul>\n",
    "    <li>Can be much more accurate than the mean, median or most frequent imputation methods (It depends on the dataset)\n",
    "</ul>\n",
    "<strong>Cons</strong><ul>\n",
    "    <li>Computationally expensive because KNN works by storing the whole training dataset in memory\n",
    "    <li>K-NN is quite sensitive to outliers in the data (unlike SVM)\n",
    "</ul>\n",
    "<strong>How it works</strong><ul>\n",
    "    <li>It creates a basic mean impute then uses the resulting complete list to construct a KDTree\n",
    "    <li>Then, it uses the resulting KDTree to compute nearest neighbours (NN)\n",
    "    <li>After it finds the k-NNs, it takes the weighted average of them\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count    13580.000000\nmean         1.610075\nstd          0.960433\nmin          0.000000\n25%          1.000000\n50%          2.000000\n75%          2.000000\nmax         10.000000\nName: Car, dtype: float64"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df_test = df.copy()\n",
    "imputer = KNNImputer(missing_values=np.nan, n_neighbors=5)\n",
    "imputer.fit(df_test[['Car']])\n",
    "df_test[['Car']] = imputer.transform(df_test[['Car']])\n",
    "df_test.Car.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>6. Imputation using <i>Multivariate Imputation by Chained Equation</i> (MICE)</h2>\n",
    "<strong>How it works</strong><ul>\n",
    "    <li>This type of imputation works by filling the missing data multiple times\n",
    "    <li>Multiple Imputations (MIs) are much better than a single imputation as it measures the uncertainty of the missing values in a better way\n",
    "    <li>The chained equations approach is also very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count    13580.000000\nmean         1.610075\nstd          0.960433\nmin          0.000000\n25%          1.000000\n50%          2.000000\n75%          2.000000\nmax         10.000000\nName: Car, dtype: float64"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "df_test = df.copy()\n",
    "imputer = IterativeImputer(missing_values=np.nan, max_iter=10, random_state=0)\n",
    "imputer.fit(df_test[['Car']])\n",
    "df_test[['Car']] = imputer.transform(df_test[['Car']])\n",
    "df_test.Car.describe(include='all')"
   ]
  }
 ]
}