{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<img src=\"../../../../images/dt.png\" style=\"background:white; display: block; margin-left: auto;margin-right: auto; width:80%\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "<h2>1. Importing the Dataset</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   Age  EstimatedSalary  Purchased\n0   19            19000          0\n1   35            20000          0\n2   26            43000          0\n3   27            57000          0\n4   19            76000          0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>EstimatedSalary</th>\n      <th>Purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>19000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35</td>\n      <td>20000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>43000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27</td>\n      <td>57000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19</td>\n      <td>76000</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../../../../data/clean/Social_Network_Ads.csv')\n",
    "display(df.head())\n",
    "x = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "source": [
    "---\n",
    "<h2>2. Splitting the Dataset</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train dataset size : 320 observations\ntest dataset size : 80 observations\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"train dataset size : {} observations\\ntest dataset size : {} observations\".format(x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "source": [
    "---\n",
    "<h2>3. Feature Scaling</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stand_x = StandardScaler().fit(x_train)\n",
    "x_ss = stand_x.transform(x_train)"
   ]
  },
  {
   "source": [
    "---\n",
    "<h2>4. Training the Model with Train Dataset</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DecisionTreeClassifier(criterion='entropy')"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "'''\n",
    "> 'criterion' — [”gini”,”entropy”], optional (default=”gini”)\n",
    "    - “gini” for the Gini impurity\n",
    "    - Gini Impurity measures the divergences between the probability distributions of the\n",
    "      target attribute’s values and splits a node such that it gives the least amount of impurity\n",
    "    - “entropy” for the information gain\n",
    "    - Information gain uses the entropy measure as the impurity measure and splits a node such that\n",
    "      it gives the most amount of information gain\n",
    "    - “entropy” might be a little slower to compute because it requires you to compute a logarithmic function\n",
    "> 'splitter' — [“best”, “random”], optional (default=”best”)\n",
    "    - “best” splitter evaluates all splits using the criterion before splitting\n",
    "    - “best” will calculate the best features to split based on the impurity measure and use that to split the nodes\n",
    "      which is perfectly fit if we have hundreds of features\n",
    "    - “random” splitter uses a random uniform function with min_feature_value, max_feature_value and random_state as inputs\n",
    "    - “random” doesn’t have the computational overhead of computing the optimal split\n",
    "    -  if our model is overfitting, then we can change the splitter to “random” and retrain\n",
    "> 'max_depth' — int or None, optional (default=None)\n",
    "    - This indicates how deep the tree can be\n",
    "    - The deeper the tree, the more splits it has and it captures more information about the data\n",
    "    - The default value (None) will often result in over-fitted decision trees!!!\n",
    "    - if our model is overfitting, reducing the number for max_depth is one way to combat overfitting\n",
    "    - It is also bad to have a very low depth because our model will underfit and experiment is the best way to find the best value\n",
    "    - 'max_depth' is tied with 'min_samples_split' and 'min_samples_leaf' parameters\n",
    "> 'min_samples_split' — int, float, optional (default=2)\n",
    "    - 'min_samples_split' talks about an internal node and by definition an internal node can have further split\n",
    "    - When we increase this parameter, the tree becomes more constrained as it has to consider more samples at each node\n",
    "    - The ideal 'min_samples_split' values tend to be between 1 to 40\n",
    "    - It is used to control over-fitting\n",
    "    - Too high values can also lead to under-fitting hence depending on the level of underfitting or overfitting\n",
    "    - 'min_samples_split' and 'min_samples_leaf' are the most responsible for the performance of the final trees from their relative importance analysis\n",
    "> 'min_samples_leaf' — int, float, optional (default=1)\n",
    "    - 'min_samples_leaf' talks about an external node and by definition is a node without any children\n",
    "    - It is always guaranteed no matter the 'min_samples_split' value\n",
    "    - It is also used to control over-fitting by defining that each leaf has more than one element\n",
    "    - The ideal 'min_samples_leaf' values tend to be between 1 to 20\n",
    "    - A very small number will usually mean the tree will overfit\n",
    "    - Increasing this value may cause underfitting\n",
    "    - 'min_samples_split' and 'min_samples_leaf' are the most responsible for the performance of the final trees from their relative importance analysis\n",
    "> 'max_features' — int, float or [“auto”, “sqrt”, “log2”], default=None\n",
    "    - If “auto”, then max_features=sqrt(n_features)\n",
    "    - If “sqrt”, then max_features=sqrt(n_features)\n",
    "    - If “log2”, then max_features=log2(n_features)\n",
    "        - if we have a high computational cost or we have a lot of overfitting, you can try with “log2”\n",
    "        - we can either bring it slightly up using sqrt\n",
    "        - or take it down further using a custom float value\n",
    "    - If None, then max_features=n_features\n",
    "    - Another use of 'max_features' is to limit overfitting\n",
    "'''\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(x_ss, y_train)"
   ]
  },
  {
   "source": [
    "---\n",
    "<h2>5. Predicting the Test Dataset and Display Results</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   y actual  y prediction\n0         1             1\n1         0             0\n2         0             0\n3         0             1\n4         0             0\n5         1             1\n6         0             0\n7         1             1\n8         0             1\n9         0             0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y actual</th>\n      <th>y prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "y_pred = dt.predict(stand_x.transform(x_test))\n",
    "\n",
    "pd.DataFrame(data=np.stack((y_test, y_pred), axis=1),\n",
    "             index=None, columns=['y actual', 'y prediction'],\n",
    "             copy=False).head(10)"
   ]
  },
  {
   "source": [
    "---\n",
    "<h2>6. Making the Confusion Matrix</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[45  6]\n [ 3 26]]\n\nConfusion matrix result shows that:\n\t- 45 correct predictions of the class 0 (who didn't buy the product)        \n\t- 6 incorrect predictions of the class 1 (predicted as user who bought the product but in reality not to)        \n\t- 26 correct predictions of the class 1 (who bought the product)        \n\t- 3 incorrect predictions of the class 0 (predicted as user who didn't buy the product but in reality they bought the product)\n"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nConfusion matrix result shows that:\\n\\t- 45 correct predictions of the class 0 (who didn\\'t buy the product)\\\n",
    "        \\n\\t- 6 incorrect predictions of the class 1 (predicted as user who bought the product but in reality not to)\\\n",
    "        \\n\\t- 26 correct predictions of the class 1 (who bought the product)\\\n",
    "        \\n\\t- 3 incorrect predictions of the class 0 (predicted as user who didn\\'t buy the product but in reality they bought the product)\")"
   ]
  }
 ]
}